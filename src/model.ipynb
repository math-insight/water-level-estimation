{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import max_error\n",
    "from matplotlib.dates import DateFormatter, MonthLocator, YearLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_pickle(r\"../data/B00020S.pkl\")\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tworzenie zbioru treningowego i testowego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Słownik ze stacjami i odpowiadającymi im przesunięciom\n",
    "stations_offset_values_dict = {\n",
    "    'GŁOGÓW': 0,\n",
    "    'ŚCINAWA': 1,\n",
    "    'MALCZYCE': 1,\n",
    "    'BRZEG DOLNY': 1,\n",
    "    'OŁAWA': 2,\n",
    "    'BRZEG': 2,\n",
    "    'RACIBÓRZ-MIEDONIA': 3,\n",
    "    'KRZYŻANOWICE': 3,\n",
    "    'OLZA': 3,\n",
    "    'CHAŁUPKI': 3\n",
    "}\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Lista różnic w dniach między Głogowem a kolejną stacją\n",
    "offset_values = [stations_offset_values_dict[station] for station in stations]\n",
    "\n",
    "# Rok, od którego chcemy trenować i testować model\n",
    "start_year = 2019\n",
    "\n",
    "# Filtrowanie stacji i daty\n",
    "condition = ((data['Date'].dt.year >= start_year) & (data['Station'].isin(stations)))\n",
    "data_filtered = data[condition]\n",
    "\n",
    "# Data frame z kolumnami: Date i osobna kolumna dla każdej stacji zawierająca średni poziom wody w danym dniu\n",
    "station_data = pd.pivot_table(data_filtered, index=['Date'], columns=['Station'], values='B00020S', aggfunc='mean').reset_index()\n",
    "\n",
    "# Zmiana kolejności kolumn\n",
    "new_column_order = ['Date'] + stations\n",
    "station_data = station_data[new_column_order]\n",
    "\n",
    "# Przesunięcie każdej kolumny o ustaloną liczbę wierszy\n",
    "for i, col in enumerate(station_data.columns[1:]):\n",
    "    station_data[col] = temp[col].shift(periods=-offset_values[i])\n",
    "    \n",
    "# Usunięcia NA\n",
    "station_data = station_data.dropna()\n",
    "\n",
    "# Zmienne służące do podziału zbioru danych na zbiór treningowy i zbiór testujący\n",
    "test_proportion = 0.2\n",
    "train_proportion = 1 - test_proportion\n",
    "split_point = int(len(station_data) * train_proportion)\n",
    "\n",
    "# Podział na zbiór treningowy i zbiór testowy\n",
    "train_data, test_data = station_data.iloc[:split_point], station_data.iloc[split_point:]\n",
    "\n",
    "# Zmienne niezależne\n",
    "x = train_data.iloc[:, 2:]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Zmienna zależna\n",
    "y = train_data.iloc[:, 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dopasowanie modelu i predykcja"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dopasowanie modelu\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# Podsumowanie\n",
    "print(model.summary())\n",
    "\n",
    "# Przewidywanie wartości na podstawie modelu\n",
    "model_prediction = model.predict(x)\n",
    "\n",
    "# To samo co wyżej, ale ręcznie\n",
    "test_predict = model.params['const'] + sum(model.params[param] * test_data[param] for param in model.params.index[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resztki modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataframe z resztami modelu\n",
    "residuals = pd.DataFrame(model.resid)\n",
    "\n",
    "# Okno wykresów\n",
    "fig, axes = plt.subplots(figsize=(36,32), ncols=1, nrows=2, gridspec_kw={'hspace': 0.3})\n",
    "months_locator = MonthLocator()\n",
    "months_fmt = DateFormatter(\"%b\")\n",
    "years_locator = YearLocator()\n",
    "years_fmt = DateFormatter(\"%Y\")\n",
    "\n",
    "# Wykres reszt\n",
    "axes[0].plot(station_data['Date'][:len(train_data)],residuals.iloc[:,0], linewidth=3)\n",
    "axes[0].set_xlim(station_data['Date'][:len(train_data)].min(), station_data['Date'][:len(train_data)].max())\n",
    "axes[0].set_xlabel('Data', fontsize=20)\n",
    "axes[0].set_title('Reszty', fontsize=30)\n",
    "axes[0].grid()\n",
    "axes[0].xaxis.set_minor_locator(months_locator)\n",
    "axes[0].xaxis.set_minor_formatter(months_fmt)\n",
    "axes[0].xaxis.set_major_locator(years_locator)\n",
    "axes[0].xaxis.set_major_formatter(years_fmt)\n",
    "axes[0].tick_params(axis='both', which='minor', labelsize=15)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.setp(axes[0].xaxis.get_minorticklabels(), rotation=45)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Wykres gęstości jądra reszt / rozkład prawdopodobieństwa reszt\n",
    "residuals.plot(kind='kde', ax=axes[1], linewidth=4)\n",
    "axes[1].set_xlim(-100, 100)\n",
    "axes[1].set_ylim(0, 0.03)\n",
    "axes[1].set_title('Wykres gęstości jądra / rozkładu prawdopodobieństwa reszt', fontsize=30)\n",
    "axes[1].set_ylabel('Gęstość', fontsize=20)\n",
    "axes[1].tick_params(axis='both', labelsize=15)\n",
    "axes[1].grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Statystyki opisowe reszt\n",
    "print(residuals.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metryki błędu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prawdziwe dane testowe\n",
    "y_true = test_data['GŁOGÓW']\n",
    "\n",
    "# Błąd średniokwadratowy\n",
    "mse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=True)\n",
    "\n",
    "# Pierwiastek z błędu kwadratowego\n",
    "rmse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=False)\n",
    "\n",
    "# Błąd procentowy średniokwadratowy\n",
    "mape = mean_absolute_percentage_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd bezwzględny\n",
    "max_absolute_error = max_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd względny\n",
    "max_relative_error = max(abs((train_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW']))\n",
    "\n",
    "# Bezwzględny błąd treningowy\n",
    "train_absolute_error = abs(train_data['GŁOGÓW'] - model_prediction)\n",
    "\n",
    "# Względny błąd treningowy\n",
    "train_relative_error = abs((train_data['GŁOGÓW'] - model_prediction) / train_data['GŁOGÓW'])\n",
    "\n",
    "# Bezwzględny błąd testowy\n",
    "test_absolute_error = abs(test_data['GŁOGÓW'] - test_predict)\n",
    "\n",
    "# Względny błąd testowy\n",
    "test_relative_error = abs((test_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW'])\n",
    "\n",
    "# Wyświetlenie niektórych powyższych metryk\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test MAPE: %.3f' % mape)\n",
    "print('Test max error: %.3f' % max_absolute_error)\n",
    "print('Test max relative error: %.5f' % max_relative_error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wykresy predykcji, błędu bezwzględnego i błędu względnego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funkcja do ustawiania własności wykresu\n",
    "def set_common_plot_properties(axes_id, title, y_label):\n",
    "    axes[axes_id].set_xlabel('Data', fontsize=15)\n",
    "    axes[axes_id].set_ylabel(y_label, fontsize=15)\n",
    "    axes[axes_id].set_xlim(train_dates.min(), test_dates.max())\n",
    "    axes[axes_id].set_title(title, fontsize=30)\n",
    "    axes[axes_id].legend(loc='upper right', fontsize=20)\n",
    "    axes[axes_id].grid()\n",
    "    axes[axes_id].xaxis.set_minor_locator(months_locator)\n",
    "    axes[axes_id].xaxis.set_minor_formatter(months_fmt)\n",
    "    axes[axes_id].xaxis.set_major_locator(years_locator)\n",
    "    axes[axes_id].xaxis.set_major_formatter(years_fmt)\n",
    "    axes[axes_id].tick_params(axis='both', which='minor', labelsize=15)\n",
    "    axes[axes_id].tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.setp(axes[axes_id].xaxis.get_minorticklabels(), rotation=45)\n",
    "    plt.setp(axes[axes_id].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# Osie X\n",
    "train_dates = station_data['Date'][:len(train_data)]\n",
    "test_dates = station_data['Date'][len(train_data):len(train_data) + len(test_data)]\n",
    "\n",
    "# Okno wykresów\n",
    "fig, axes = plt.subplots(figsize=(36, 32), nrows=3, ncols=1, gridspec_kw={'hspace': 0.3})\n",
    "months_locator = MonthLocator()\n",
    "months_fmt = DateFormatter(\"%b\")\n",
    "years_locator = YearLocator()\n",
    "years_fmt = DateFormatter(\"%Y\")\n",
    "\n",
    "# Wykres predykcji\n",
    "axes[0].plot(train_dates, train_data['GŁOGÓW'], color='b', label='Faktyczne dane', linewidth=2)\n",
    "axes[0].plot(train_dates, model_prediction, 'r', label='Model', linewidth=2)\n",
    "axes[0].plot(test_dates, test_data['GŁOGÓW'], color='g', label='Faktyczne dane (test)', linewidth=2)\n",
    "axes[0].plot(test_dates, test_predict, color='m', label='Model (test)', linewidth=2)\n",
    "set_common_plot_properties(0, 'Predykcja poziomu wody w Głogowie', 'Poziom wody (cm)')\n",
    "\n",
    "# Wykres błędu bezwzględnego\n",
    "axes[1].plot(train_dates, train_absolute_error, color='b', label='Błąd bezwzględny', linewidth=2)\n",
    "axes[1].plot(test_dates, test_absolute_error, color='r', label='Błąd bezwzględny (test)', linewidth=2)\n",
    "set_common_plot_properties(1, 'Błąd bezwzględny', 'Błąd bezwzględny')\n",
    "\n",
    "# Wykres błędu względnego\n",
    "axes[2].plot(train_dates, train_relative_error, color='b', label='Błąd względny', linewidth=2)\n",
    "axes[2].plot(test_dates, test_relative_error, color='r', label='Błąd względny (test)', linewidth=2)\n",
    "set_common_plot_properties(2, 'Błąd względny', 'Błąd względny')\n",
    "\n",
    "# Wyświetlenie wykresów\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Korelacje Pearsona między stacjami"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funkcja do liczenia korelacji Pearsona między dwiema stacjami\n",
    "def correlation_between_stations(station_1, station_2, lag):\n",
    "    station_1_id = stations.index(station_1.upper())\n",
    "    station_2_id = stations.index(station_2.upper())\n",
    "    correlation = lagged_dfs[lag][station_1_id]['B00020S'].corr(lagged_dfs[0][station_2_id]['B00020S'])\n",
    "    return round(correlation, 3)\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Grupowanie po dniach i stacjach\n",
    "data_grouped = data.groupby(['Date', 'Station'])['B00020S'].mean().reset_index()\n",
    "\n",
    "# Maksymalny lag\n",
    "max_lag = 7\n",
    "\n",
    "# Lista od 0 do max_lag\n",
    "lags = range(max_lag + 1)\n",
    "\n",
    "# Dwuwymiarowa lista zlagowanych dataframe'ów stacji\n",
    "# Dostęp do wybranej stacji i lagu: lagged_dfs[lag][id_stacji np. 0 dla Głogowa]\n",
    "start_date = '2008-01-08'\n",
    "end_date = '2023-09-30'\n",
    "lagged_dfs =[[data_grouped[(data_grouped['Date']\n",
    "                .between(\n",
    "                    pd.to_datetime(start_date) - pd.DateOffset(days=lag),\n",
    "                    pd.to_datetime(end_date) - pd.DateOffset(days=lag)\n",
    "                ))&(data_grouped['Station'] == station)].reset_index(drop=True) for station in stations]for lag in lags\n",
    "             ]\n",
    "\n",
    "# Lista stacji, dla których chcemy obliczyć korelację Pearsona względem Głogowa\n",
    "stations_to_calculate_corr = stations\n",
    "\n",
    "# Wyświetlanie korelacji Pearsona\n",
    "for station in stations_to_calculate_corr:\n",
    "    print(f\"\\nWspółczynnik korelacji Pearsona liczony na podstawie poziomu wody w stacjach: {station.capitalize()} - Głogów\")\n",
    "    for lag in lags:\n",
    "        print(f\"Lag: {lag}, p = {correlation_between_stations(station, 'Głogów', lag)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
