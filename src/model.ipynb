{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import max_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_pickle(r\"../data/B00020S.pkl\")\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    " - **DONE** znaleźć prawidłowe offset_values — Max\n",
    " - **DONE** wykresy błędów — Adam\n",
    " - **DONE** nałożyć wartość bezwzględną na histogram błędu bezwzględnego — Adam\n",
    " - **DONE** upiększyć wykresy — Max\n",
    " - dodać do temp kolumnę z datami — Max\n",
    " - może jakiś opis matematyczny w markdownie tej regresji czy coś — Max albo Adam\n",
    " - **DONE** zautomatyzować dobór offset_values - Max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Słownik ze stacjami i odpowiadającymi im przesunięciom\n",
    "stations_offset_values_dict = {\n",
    "    'GŁOGÓW': 0,\n",
    "    'ŚCINAWA': 1,\n",
    "    'MALCZYCE': 1,\n",
    "    'BRZEG DOLNY': 1,\n",
    "    'OŁAWA': 2,\n",
    "    'BRZEG': 2,\n",
    "    'RACIBÓRZ-MIEDONIA': 3,\n",
    "    'KRZYŻANOWICE': 3,\n",
    "    'OLZA': 3,\n",
    "    'CHAŁUPKI': 3\n",
    "}\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Lista różnic w dniach między Głogowem a kolejną stacją\n",
    "offset_values = [stations_offset_values_dict[station] for station in stations]\n",
    "\n",
    "# Rok, od którego chcemy trenować i testować model\n",
    "start_year = 2019\n",
    "\n",
    "# Grupowanie po dniach i stacjach\n",
    "data_grouped = data.groupby(['Date', 'Station'])['B00020S'].mean().reset_index()\n",
    "\n",
    "# Osobny dataframe dla każdej stacji i tylko rekordy od danego roku\n",
    "station_datas = [data_grouped[(data_grouped['Station'] == station) & (data_grouped['Date'].dt.year >= start_year)].reset_index() for station in stations]\n",
    "\n",
    "# Połączenie kolumn z poziomem wody z każdej stacji w jeden dataframe\n",
    "temp = pd.concat([station_data['B00020S'] for station_data in station_datas], axis='columns').reset_index(drop=True)\n",
    "\n",
    "# Zmiana nazw kolumn\n",
    "temp.columns = stations\n",
    "\n",
    "# Przesunięcie każdej kolumny o odpowiednią liczbę dni\n",
    "for i, col in enumerate(temp.columns):\n",
    "    temp[col] = temp[col].shift(periods=-offset_values[i])\n",
    "\n",
    "# Usunięcie NA, możliwe, że później można to dopracować\n",
    "temp = temp.dropna()\n",
    "\n",
    "# Zmienne służące do podziału zbioru danych na zbiór treningowy i zbiór testujący\n",
    "test_proportion = 0.2\n",
    "train_proportion = 1 - test_proportion\n",
    "split_point = int(len(temp) * train_proportion)\n",
    "\n",
    "# Podział na zbiór treningowy i zbiór testowy\n",
    "train_data, test_data = temp.iloc[:split_point], temp.iloc[split_point:]\n",
    "\n",
    "# Zmienne niezależne\n",
    "x = train_data.iloc[:, 1:]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Zmienna niezależna\n",
    "y = train_data.iloc[:, 0]\n",
    "\n",
    "# Dopasowanie modelu\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# Podsumowanie\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resztki modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataframe z resztami modelu\n",
    "residuals = pd.DataFrame(model.resid)\n",
    "\n",
    "# Wykres reszt\n",
    "fig, axes = plt.subplots(figsize=(36,32), ncols=1, nrows=2, gridspec_kw={'hspace': 0.3})\n",
    "residuals.plot(ax = axes[0], linewidth=3)\n",
    "axes[0].set_xlim(0, len(residuals))\n",
    "axes[0].set_title('Reszty', fontsize=30)\n",
    "axes[0].legend().set_visible(False)\n",
    "axes[0].grid()\n",
    "\n",
    "# Wykres gęstości jądra reszt / rozkład prawdopodobieństwa reszt\n",
    "residuals.plot(kind='kde', ax=axes[1], linewidth=4)\n",
    "axes[1].set_xlim(-100, 100)\n",
    "axes[1].set_ylim(0, 0.03)\n",
    "axes[1].set_title('Wykres gęstości jądra / rozkładu prawdopodobieństwa reszt', fontsize=30)\n",
    "axes[1].set_ylabel('Gęstość', fontsize=20)\n",
    "axes[1].legend().set_visible(False)\n",
    "axes[1].grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Statystyki opisowe reszt\n",
    "print(residuals.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predykcja modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Przewidywanie wartości na podstawie modelu\n",
    "model.predict(x)\n",
    "\n",
    "# To samo co wyżej, ale ręcznie\n",
    "test_predict = model.params['const'] + sum(model.params[param] * test_data[param] for param in model.params.index[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metryki błędu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prawdziwe dane testowe\n",
    "y_true = test_data.iloc[:, 0]\n",
    "\n",
    "# Błąd średniokwadratowy\n",
    "mse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=True)\n",
    "\n",
    "# Pierwiastek z błędu kwadratowego\n",
    "rmse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=False)\n",
    "\n",
    "# Błąd procentowy średniokwadratowy\n",
    "mape = mean_absolute_percentage_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd bezwzględny\n",
    "max_absolute_error = max_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd względny\n",
    "max_relative_error = max(abs((train_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW']))\n",
    "\n",
    "# Bezwzględny błąd treningowy\n",
    "train_absolute_error = abs(train_data['GŁOGÓW'] - test_predict)\n",
    "\n",
    "# Względny błąd treningowy\n",
    "train_relative_error = abs((train_data['GŁOGÓW'] - model.predict()) / train_data['GŁOGÓW'])\n",
    "\n",
    "# Bezwzględny błąd testowy\n",
    "test_absolute_error = abs(test_data['GŁOGÓW'] - test_predict)\n",
    "\n",
    "# Względny błąd testowy\n",
    "test_relative_error = abs((test_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW'])\n",
    "\n",
    "# Wyświetlenie niektórych powyższych metryk\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test MAPE: %.3f' % mape)\n",
    "print('Test max error: %.3f' % max_absolute_error)\n",
    "print('Test max relative error: %.5f' % max_relative_error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wykresy predykcji, błędu bezwzględnego i błędu względnego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tymczasowa oś X, trzeba będzie zmienić na prawdziwą datę\n",
    "date_train = np.arange(0, len(train_data))\n",
    "date_test = np.arange(len(train_data), len(train_data) + len(test_data))\n",
    "\n",
    "# Okno wykresów\n",
    "fig, axes = plt.subplots(figsize=(36, 32), nrows=3, ncols=1, gridspec_kw={'hspace': 0.3})\n",
    "\n",
    "# Wykres predykcji\n",
    "axes[0].plot(date_train, train_data['GŁOGÓW'], color='b', label='Faktyczne dane', linewidth=3)\n",
    "axes[0].plot(date_train, model.predict(), 'r', label='Model', linewidth=3)\n",
    "axes[0].plot(date_test, test_data['GŁOGÓW'], color='r', label='Faktyczne dane (test)', linewidth=3)\n",
    "axes[0].plot(date_test, test_predict, color='m', label='Model (test)', linewidth=3)\n",
    "axes[0].set_xlabel('Data', fontsize=15)\n",
    "axes[0].set_ylabel('Poziom wody (cm)', fontsize=15)\n",
    "axes[0].set_title('Predykcja poziomu wody w Głogowie', fontsize=30)\n",
    "axes[0].legend(loc='upper right', fontsize=20)\n",
    "axes[0].grid()\n",
    "\n",
    "# Wykres błędu bezwzględnego\n",
    "axes[1].plot(date_train, train_absolute_error, color='b', label='Błąd bezwzględny', linewidth=3) # ValueError: x and y must have same first dimension, but have shapes (1383,) and (1729,)\n",
    "axes[1].plot(date_test, test_absolute_error, color='r', label='Błąd bezwzględny (test)', linewidth=3)\n",
    "axes[1].set_xlabel('Data', fontsize=15)\n",
    "axes[1].set_ylabel('Błąd bezwzględny', fontsize=15)\n",
    "axes[1].set_title('Błąd bezwzględny', fontsize=30)\n",
    "axes[1].legend(loc='upper right', fontsize=20)\n",
    "axes[1].grid()\n",
    "\n",
    "# Wykres błędu względnego\n",
    "axes[2].plot(date_train, train_relative_error, color='b', label='Błąd względny', linewidth=3)\n",
    "axes[2].plot(date_test, test_relative_error, color='r', label='Błąd względny (test)', linewidth=3)\n",
    "axes[2].set_xlabel('Data', fontsize=15)\n",
    "axes[2].set_ylabel('Błąd względny', fontsize=15)\n",
    "axes[2].set_title('Błąd względny', fontsize=30)\n",
    "axes[2].legend(loc='upper right', fontsize=20)\n",
    "axes[2].grid()\n",
    "\n",
    "# Wyświetlenie wykresów\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Korelacje Pearsona między stacjami"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funkcja do liczenia korelacji Pearsona między dwiema stacjami\n",
    "def correlation_between_stations(station_1, station_2, lag):\n",
    "    station_1_id = stations.index(station_1.upper())\n",
    "    station_2_id = stations.index(station_2.upper())\n",
    "    correlation = lagged_dfs[lag][station_1_id]['B00020S'].corr(lagged_dfs[0][station_2_id]['B00020S'])\n",
    "    return round(correlation, 3)\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Grupowanie po dniach i stacjach\n",
    "data_grouped = data.groupby(['Date', 'Station'])['B00020S'].mean().reset_index()\n",
    "\n",
    "# Maksymalny lag\n",
    "max_lag = 7\n",
    "\n",
    "# Lista od 0 do max_lag\n",
    "lags = range(max_lag + 1)\n",
    "\n",
    "# Dwuwymiarowa lista zlagowanych dataframe'ów stacji\n",
    "# Dostęp do wybranej stacji i lagu: lagged_dfs[lag][id_stacji np. 0 dla Głogowa]\n",
    "start_date = '2008-01-08'\n",
    "end_date = '2023-09-30'\n",
    "lagged_dfs =[[data_grouped[(data_grouped['Date']\n",
    "                .between(\n",
    "                    pd.to_datetime(start_date) - pd.DateOffset(days=lag),\n",
    "                    pd.to_datetime(end_date) - pd.DateOffset(days=lag)\n",
    "                ))&(data_grouped['Station'] == station)].reset_index(drop=True) for station in stations]for lag in lags\n",
    "             ]\n",
    "# Lista stacji, dla których chcemy obliczyć korelację Pearsona względem Głogowa\n",
    "stations_to_calculate_corr = ['ŚCINAWA', 'MALCZYCE']\n",
    "\n",
    "# Wyświetlanie korelacji Pearsona\n",
    "for station in stations_to_calculate_corr:\n",
    "    print(f\"\\nWspółczynnik korelacji Pearsona liczony na podstawie poziomu wody w stacjach: {station.capitalize()} - Głogów\")\n",
    "    for lag in lags:\n",
    "        print(f\"Lag: {lag}, p = {correlation_between_stations(station, 'Głogów', lag)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
