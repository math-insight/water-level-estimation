{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-21T10:02:09.950737500Z",
     "start_time": "2023-12-21T10:01:39.211894500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import max_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Jeżeli data nie jeszcze zostało wczytane, wczytaj\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mdata\u001B[49m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m():\n\u001B[0;32m      3\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_pickle(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/B00020S.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m     data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(r\"../data/B00020S.pkl\")\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T10:02:18.004782400Z",
     "start_time": "2023-12-21T10:02:15.341461600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    " - **DONE** znaleźć prawidłowe offset_values — Max\n",
    " - **DONE** wykresy błędów — Adam\n",
    " - **DONE** nałożyć wartość bezwzględną na histogram błędu bezwzględnego — Adam\n",
    " - **DONE** upiększyć wykresy — Max\n",
    " - dodać do temp kolumnę z datami — Max\n",
    " - może jakiś opis matematyczny w markdownie tej regresji czy coś — Max albo Adam\n",
    " - **DONE** zautomatyzować dobór offset_values - Max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 GŁOGÓW   R-squared:                       0.932\n",
      "Model:                            OLS   Adj. R-squared:                  0.932\n",
      "Method:                 Least Squares   F-statistic:                     2095.\n",
      "Date:                Wed, 20 Dec 2023   Prob (F-statistic):               0.00\n",
      "Time:                        21:02:32   Log-Likelihood:                -6066.1\n",
      "No. Observations:                1383   AIC:                         1.215e+04\n",
      "Df Residuals:                    1373   BIC:                         1.220e+04\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const               140.0133      4.451     31.458      0.000     131.282     148.744\n",
      "ŚCINAWA               0.7098      0.015     47.858      0.000       0.681       0.739\n",
      "MALCZYCE              0.2132      0.018     12.133      0.000       0.179       0.248\n",
      "BRZEG DOLNY          -0.0178      0.012     -1.508      0.132      -0.041       0.005\n",
      "OŁAWA                -0.1535      0.047     -3.276      0.001      -0.245      -0.062\n",
      "BRZEG                 0.0631      0.038      1.641      0.101      -0.012       0.139\n",
      "RACIBÓRZ-MIEDONIA    -0.0189      0.017     -1.114      0.266      -0.052       0.014\n",
      "KRZYŻANOWICE         -0.1442      0.061     -2.347      0.019      -0.265      -0.024\n",
      "OLZA                  0.1072      0.043      2.490      0.013       0.023       0.192\n",
      "CHAŁUPKI              0.0055      0.068      0.081      0.935      -0.127       0.138\n",
      "==============================================================================\n",
      "Omnibus:                      190.343   Durbin-Watson:                   0.880\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2304.800\n",
      "Skew:                          -0.065   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.323   Cond. No.                     4.88e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.88e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Słownik ze stacjami i odpowiadającymi im przesunięciom\n",
    "stations_offset_values_dict = {\n",
    "    'GŁOGÓW': 0,\n",
    "    'ŚCINAWA': 1,\n",
    "    'MALCZYCE': 1,\n",
    "    'BRZEG DOLNY': 1,\n",
    "    'OŁAWA': 2,\n",
    "    'BRZEG': 2,\n",
    "    'RACIBÓRZ-MIEDONIA': 3,\n",
    "    'KRZYŻANOWICE': 3,\n",
    "    'OLZA': 3,\n",
    "    'CHAŁUPKI': 3\n",
    "}\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Lista różnic w dniach między Głogowem a kolejną stacją\n",
    "offset_values = [stations_offset_values_dict[station] for station in stations]\n",
    "\n",
    "# Rok, od którego chcemy trenować i testować model\n",
    "start_year = 2019\n",
    "\n",
    "# Grupowanie po dniach i stacjach\n",
    "data_grouped = data.groupby(['Date', 'Station'])['B00020S'].mean().reset_index()\n",
    "\n",
    "# Osobny dataframe dla każdej stacji i tylko rekordy od danego roku\n",
    "station_datas = [data_grouped[(data_grouped['Station'] == station) & (data_grouped['Date'].dt.year >= start_year)].reset_index() for station in stations]\n",
    "\n",
    "# Połączenie kolumn z poziomem wody z każdej stacji w jeden dataframe\n",
    "temp = pd.concat([station_data['B00020S'] for station_data in station_datas], axis='columns').reset_index(drop=True)\n",
    "\n",
    "# Zmiana nazw kolumn\n",
    "temp.columns = stations\n",
    "\n",
    "# Przesunięcie każdej kolumny o odpowiednią liczbę dni\n",
    "for i, col in enumerate(temp.columns):\n",
    "    temp[col] = temp[col].shift(periods=-offset_values[i])\n",
    "\n",
    "# Usunięcie NA, możliwe, że później można to dopracować\n",
    "temp = temp.dropna()\n",
    "\n",
    "# Zmienne służące do podziału zbioru danych na zbiór treningowy i zbiór testujący\n",
    "test_proportion = 0.2\n",
    "train_proportion = 1 - test_proportion\n",
    "split_point = int(len(temp) * train_proportion)\n",
    "\n",
    "# Podział na zbiór treningowy i zbiór testowy\n",
    "train_data, test_data = temp.iloc[:split_point], temp.iloc[split_point:]\n",
    "\n",
    "# Zmienne niezależne\n",
    "x = train_data.iloc[:, 1:]\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Zmienna niezależna\n",
    "y = train_data.iloc[:, 0]\n",
    "\n",
    "# Dopasowanie modelu\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# Podsumowanie\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:02:32.517698100Z",
     "start_time": "2023-12-20T20:02:19.284567200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resztki modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataframe z resztami modelu\n",
    "residuals = pd.DataFrame(model.resid)\n",
    "\n",
    "# Wykres reszt\n",
    "fig, axes = plt.subplots(figsize=(36,32), ncols=1, nrows=2, gridspec_kw={'hspace': 0.3})\n",
    "residuals.plot(ax = axes[0], linewidth=3)\n",
    "axes[0].set_xlim(0, len(residuals))\n",
    "axes[0].set_title('Reszty', fontsize=30)\n",
    "axes[0].legend().set_visible(False)\n",
    "axes[0].grid()\n",
    "\n",
    "# Wykres gęstości jądra reszt / rozkład prawdopodobieństwa reszt\n",
    "residuals.plot(kind='kde', ax=axes[1], linewidth=4)\n",
    "axes[1].set_xlim(-100, 100)\n",
    "axes[1].set_ylim(0, 0.03)\n",
    "axes[1].set_title('Wykres gęstości jądra / rozkładu prawdopodobieństwa reszt', fontsize=30)\n",
    "axes[1].set_ylabel('Gęstość', fontsize=20)\n",
    "axes[1].legend().set_visible(False)\n",
    "axes[1].grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Statystyki opisowe reszt\n",
    "print(residuals.describe())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predykcja modelu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Przewidywanie wartości na podstawie modelu\n",
    "model.predict(x)\n",
    "\n",
    "# To samo co wyżej, ale ręcznie\n",
    "test_predict = model.params['const'] + sum(model.params[param] * test_data[param] for param in model.params.index[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metryki błędu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prawdziwe dane testowe\n",
    "y_true = test_data.iloc[:, 0]\n",
    "\n",
    "# Błąd średniokwadratowy\n",
    "mse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=True)\n",
    "\n",
    "# Pierwiastek z błędu kwadratowego\n",
    "rmse = mean_squared_error(y_true=y_true, y_pred=test_predict, squared=False)\n",
    "\n",
    "# Błąd procentowy średniokwadratowy\n",
    "mape = mean_absolute_percentage_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd bezwzględny\n",
    "max_absolute_error = max_error(y_true=y_true, y_pred=test_predict)\n",
    "\n",
    "# Maksymalny błąd względny\n",
    "max_relative_error = max(abs((train_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW']))\n",
    "\n",
    "# Bezwzględny błąd treningowy\n",
    "train_absolute_error = abs(train_data['GŁOGÓW'] - test_predict)\n",
    "\n",
    "# Względny błąd treningowy\n",
    "train_relative_error = abs((train_data['GŁOGÓW'] - model.predict()) / train_data['GŁOGÓW'])\n",
    "\n",
    "# Bezwzględny błąd testowy\n",
    "test_absolute_error = abs(test_data['GŁOGÓW'] - test_predict)\n",
    "\n",
    "# Względny błąd testowy\n",
    "test_relative_error = abs((test_data['GŁOGÓW'] - test_predict) / test_data['GŁOGÓW'])\n",
    "\n",
    "# Wyświetlenie niektórych powyższych metryk\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test MAPE: %.3f' % mape)\n",
    "print('Test max error: %.3f' % max_absolute_error)\n",
    "print('Test max relative error: %.5f' % max_relative_error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wykresy predykcji, błędu bezwzględnego i błędu względnego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tymczasowa oś X, trzeba będzie zmienić na prawdziwą datę\n",
    "date_train = np.arange(0, len(train_data))\n",
    "date_test = np.arange(len(train_data), len(train_data) + len(test_data))\n",
    "\n",
    "# Okno wykresów\n",
    "fig, axes = plt.subplots(figsize=(36, 32), nrows=3, ncols=1, gridspec_kw={'hspace': 0.3})\n",
    "\n",
    "# Wykres predykcji\n",
    "axes[0].plot(date_train, train_data['GŁOGÓW'], color='b', label='Faktyczne dane', linewidth=3)\n",
    "axes[0].plot(date_train, model.predict(), 'r', label='Model', linewidth=3)\n",
    "axes[0].plot(date_test, test_data['GŁOGÓW'], color='r', label='Faktyczne dane (test)', linewidth=3)\n",
    "axes[0].plot(date_test, test_predict, color='m', label='Model (test)', linewidth=3)\n",
    "axes[0].set_xlabel('Data', fontsize=15)\n",
    "axes[0].set_ylabel('Poziom wody (cm)', fontsize=15)\n",
    "axes[0].set_title('Predykcja poziomu wody w Głogowie', fontsize=30)\n",
    "axes[0].legend(loc='upper right', fontsize=20)\n",
    "axes[0].grid()\n",
    "\n",
    "# Wykres błędu bezwzględnego\n",
    "axes[1].plot(date_train, train_absolute_error, color='b', label='Błąd bezwzględny', linewidth=3) # ValueError: x and y must have same first dimension, but have shapes (1383,) and (1729,)\n",
    "axes[1].plot(date_test, test_absolute_error, color='r', label='Błąd bezwzględny (test)', linewidth=3)\n",
    "axes[1].set_xlabel('Data', fontsize=15)\n",
    "axes[1].set_ylabel('Błąd bezwzględny', fontsize=15)\n",
    "axes[1].set_title('Błąd bezwzględny', fontsize=30)\n",
    "axes[1].legend(loc='upper right', fontsize=20)\n",
    "axes[1].grid()\n",
    "\n",
    "# Wykres błędu względnego\n",
    "axes[2].plot(date_train, train_relative_error, color='b', label='Błąd względny', linewidth=3)\n",
    "axes[2].plot(date_test, test_relative_error, color='r', label='Błąd względny (test)', linewidth=3)\n",
    "axes[2].set_xlabel('Data', fontsize=15)\n",
    "axes[2].set_ylabel('Błąd względny', fontsize=15)\n",
    "axes[2].set_title('Błąd względny', fontsize=30)\n",
    "axes[2].legend(loc='upper right', fontsize=20)\n",
    "axes[2].grid()\n",
    "\n",
    "# Wyświetlenie wykresów\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Korelacje Pearsona między stacjami"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funkcja do liczenia korelacji Pearsona między dwiema stacjami\n",
    "def correlation_between_stations(station_1, station_2, lag):\n",
    "    station_1_id = stations.index(station_1.upper())\n",
    "    station_2_id = stations.index(station_2.upper())\n",
    "    correlation = lagged_dfs[lag][station_1_id]['B00020S'].corr(lagged_dfs[0][station_2_id]['B00020S'])\n",
    "    return round(correlation, 3)\n",
    "\n",
    "# Stacje\n",
    "stations = ['GŁOGÓW', 'ŚCINAWA', 'MALCZYCE', 'BRZEG DOLNY', 'OŁAWA', 'BRZEG', 'RACIBÓRZ-MIEDONIA', 'KRZYŻANOWICE', 'OLZA', 'CHAŁUPKI']\n",
    "\n",
    "# Grupowanie po dniach i stacjach\n",
    "data_grouped = data.groupby(['Date', 'Station'])['B00020S'].mean().reset_index()\n",
    "\n",
    "# Maksymalny lag\n",
    "max_lag = 7\n",
    "\n",
    "# Lista od 0 do max_lag\n",
    "lags = range(max_lag + 1)\n",
    "\n",
    "# Dwuwymiarowa lista zlagowanych dataframe'ów stacji\n",
    "# Dostęp do wybranej stacji i lagu: lagged_dfs[lag][id_stacji np. 0 dla Głogowa]\n",
    "start_date = '2008-01-08'\n",
    "end_date = '2023-09-30'\n",
    "lagged_dfs =[[data_grouped[(data_grouped['Date']\n",
    "                .between(\n",
    "                    pd.to_datetime(start_date) - pd.DateOffset(days=lag),\n",
    "                    pd.to_datetime(end_date) - pd.DateOffset(days=lag)\n",
    "                ))&(data_grouped['Station'] == station)].reset_index(drop=True) for station in stations]for lag in lags\n",
    "             ]\n",
    "# Lista stacji, dla których chcemy obliczyć korelację Pearsona względem Głogowa\n",
    "stations_to_calculate_corr = ['ŚCINAWA', 'MALCZYCE']\n",
    "\n",
    "# Wyświetlanie korelacji Pearsona\n",
    "for station in stations_to_calculate_corr:\n",
    "    print(f\"\\nWspółczynnik korelacji Pearsona liczony na podstawie poziomu wody w stacjach: {station.capitalize()} - Głogów\")\n",
    "    for lag in lags:\n",
    "        print(f\"Lag: {lag}, p = {correlation_between_stations(station, 'Głogów', lag)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "          GŁOGÓW     ŚCINAWA    MALCZYCE  BRZEG DOLNY       OŁAWA       BRZEG  \\\n0     226.270833  145.652778  145.340278   257.541667  163.298611  163.777778   \n1     231.680556  142.138889  121.152778   234.090278  164.753472  164.958333   \n2     239.347222  128.979167  127.034722   245.069444  159.663194  157.229167   \n3     237.090278  134.125000  128.604167   243.076389  161.340278  160.451389   \n4     230.763889  125.187500   99.229167   219.298611  166.434028  165.437500   \n...          ...         ...         ...          ...         ...         ...   \n1378  249.727273  132.888889  134.104167   330.187500  170.708333  173.506944   \n1379  251.034722  138.708333  124.875000   331.090909  169.555556  172.583333   \n1380  250.447552  132.833333  106.409722   329.166667  149.677083  170.534722   \n1381  239.564286  135.597222  113.888889   330.500000  150.645833  170.638889   \n1382  237.380282  138.062500   96.270833   327.340278  158.937500  169.791667   \n\n      RACIBÓRZ-MIEDONIA  KRZYŻANOWICE        OLZA    CHAŁUPKI  \n0            160.517483    125.173611  142.243056  115.875000  \n1            154.666667    122.180556  141.000000  115.590278  \n2            163.202797    128.777778  145.375000  119.791667  \n3            154.868056    119.458333  138.465278  112.861111  \n4            148.361111    115.631944  136.250000  110.944444  \n...                 ...           ...         ...         ...  \n1378         146.326389     96.715278  156.736111  103.486111  \n1379         145.187500     95.638889  156.111111  103.118056  \n1380         142.597222     93.583333  155.090909  100.527778  \n1381         142.180556     93.097222  154.611111  100.416667  \n1382         145.666667     90.798611  153.409722   99.291667  \n\n[1383 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GŁOGÓW</th>\n      <th>ŚCINAWA</th>\n      <th>MALCZYCE</th>\n      <th>BRZEG DOLNY</th>\n      <th>OŁAWA</th>\n      <th>BRZEG</th>\n      <th>RACIBÓRZ-MIEDONIA</th>\n      <th>KRZYŻANOWICE</th>\n      <th>OLZA</th>\n      <th>CHAŁUPKI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>226.270833</td>\n      <td>145.652778</td>\n      <td>145.340278</td>\n      <td>257.541667</td>\n      <td>163.298611</td>\n      <td>163.777778</td>\n      <td>160.517483</td>\n      <td>125.173611</td>\n      <td>142.243056</td>\n      <td>115.875000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>231.680556</td>\n      <td>142.138889</td>\n      <td>121.152778</td>\n      <td>234.090278</td>\n      <td>164.753472</td>\n      <td>164.958333</td>\n      <td>154.666667</td>\n      <td>122.180556</td>\n      <td>141.000000</td>\n      <td>115.590278</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>239.347222</td>\n      <td>128.979167</td>\n      <td>127.034722</td>\n      <td>245.069444</td>\n      <td>159.663194</td>\n      <td>157.229167</td>\n      <td>163.202797</td>\n      <td>128.777778</td>\n      <td>145.375000</td>\n      <td>119.791667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>237.090278</td>\n      <td>134.125000</td>\n      <td>128.604167</td>\n      <td>243.076389</td>\n      <td>161.340278</td>\n      <td>160.451389</td>\n      <td>154.868056</td>\n      <td>119.458333</td>\n      <td>138.465278</td>\n      <td>112.861111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>230.763889</td>\n      <td>125.187500</td>\n      <td>99.229167</td>\n      <td>219.298611</td>\n      <td>166.434028</td>\n      <td>165.437500</td>\n      <td>148.361111</td>\n      <td>115.631944</td>\n      <td>136.250000</td>\n      <td>110.944444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1378</th>\n      <td>249.727273</td>\n      <td>132.888889</td>\n      <td>134.104167</td>\n      <td>330.187500</td>\n      <td>170.708333</td>\n      <td>173.506944</td>\n      <td>146.326389</td>\n      <td>96.715278</td>\n      <td>156.736111</td>\n      <td>103.486111</td>\n    </tr>\n    <tr>\n      <th>1379</th>\n      <td>251.034722</td>\n      <td>138.708333</td>\n      <td>124.875000</td>\n      <td>331.090909</td>\n      <td>169.555556</td>\n      <td>172.583333</td>\n      <td>145.187500</td>\n      <td>95.638889</td>\n      <td>156.111111</td>\n      <td>103.118056</td>\n    </tr>\n    <tr>\n      <th>1380</th>\n      <td>250.447552</td>\n      <td>132.833333</td>\n      <td>106.409722</td>\n      <td>329.166667</td>\n      <td>149.677083</td>\n      <td>170.534722</td>\n      <td>142.597222</td>\n      <td>93.583333</td>\n      <td>155.090909</td>\n      <td>100.527778</td>\n    </tr>\n    <tr>\n      <th>1381</th>\n      <td>239.564286</td>\n      <td>135.597222</td>\n      <td>113.888889</td>\n      <td>330.500000</td>\n      <td>150.645833</td>\n      <td>170.638889</td>\n      <td>142.180556</td>\n      <td>93.097222</td>\n      <td>154.611111</td>\n      <td>100.416667</td>\n    </tr>\n    <tr>\n      <th>1382</th>\n      <td>237.380282</td>\n      <td>138.062500</td>\n      <td>96.270833</td>\n      <td>327.340278</td>\n      <td>158.937500</td>\n      <td>169.791667</td>\n      <td>145.666667</td>\n      <td>90.798611</td>\n      <td>153.409722</td>\n      <td>99.291667</td>\n    </tr>\n  </tbody>\n</table>\n<p>1383 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T20:05:01.481466Z",
     "start_time": "2023-12-20T20:05:01.348909200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
